import os
import requests
import time
from bing_image_urls import bing_image_urls
from PIL import Image
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
from duckduckgo_search import DDGS

# ================================
# Configuration
# ================================

categories ={
    "Ø§Ù„Ø·ÙÙˆÙ„Ø©_ÙˆØ§Ù„Ø±Ø¹Ø§ÙŠØ©": [
    "Ù…Ø±Ø§Ø­Ù„ Ù†Ù…Ùˆ Ø§Ù„Ø·ÙÙ„ Ø¨Ø§Ù„ØµÙˆØ±", 
    "Ø£Ù†Ø´Ø·Ø© ØªØ­ÙÙŠØ²ÙŠØ© Ù„Ù„Ø±Ø¶Ø¹", 
    "Ø¨Ø·Ø§Ù‚Ø§Øª Ù†ÙˆÙ… Ø§Ù„Ø·ÙÙ„", 
    "Ø±Ø¹Ø§ÙŠØ© Ø§Ù„Ø£Ù…ÙˆÙ…Ø© Ø§Ù„Ù…ØµÙˆØ±Ø©",
    "Ø¬Ø¯Ø§ÙˆÙ„ ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø¶ÙŠØ¹", 
    "ØµÙˆØ± Ø±Ø¹Ø§ÙŠØ© Ø­Ø¯ÙŠØ«ÙŠ Ø§Ù„ÙˆÙ„Ø§Ø¯Ø©", 
    "Ø¨ÙŠØ¦Ø© Ø¢Ù…Ù†Ø© Ù„Ù„Ø£Ø·ÙØ§Ù„", 
    "Ø£Ù†Ø´Ø·Ø© ØªÙ†Ù…ÙŠØ© Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ø­Ø±ÙƒÙŠØ©"
]
,
"Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯_ÙˆØ§Ù„Ù…Ø§Ù„": [
    "ØªØµÙ…ÙŠÙ…Ø§Øª ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©", 
    "Ø±Ø³Ù… Ø§Ù„Ù†Ù‚ÙˆØ¯ ÙˆØ§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©", 
    "Ø±Ø³Ù… ØªÙˆØ¶ÙŠØ­ÙŠ Ù„Ù„Ø¨Ù†ÙˆÙƒ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ©",
    "Ø¥Ù†ÙÙˆØ¬Ø±Ø§ÙÙŠÙƒ Ø§Ù„ØªØ¶Ø®Ù… ÙˆØ§Ù„Ø§Ø¯Ø®Ø§Ø±", 
    "Ø¨Ø·Ø§Ù‚Ø§Øª Ø³Ù„ÙˆÙƒ Ø§Ù„Ù…Ø³ØªÙ‡Ù„Ùƒ", 
    "Ø±Ø³ÙˆÙ… ØªÙˆØ¶ÙŠØ­ÙŠØ© Ù„Ù„ØªÙ…ÙˆÙŠÙ„ Ø§Ù„Ø´Ø®ØµÙŠ", 
    "Ù…Ø®Ø·Ø·Ø§Øª Ø§Ù„ÙÙ‚Ø± ÙˆØ§Ù„Ø±ÙØ§Ù‡", 
    "ØµÙˆØ± Ø§Ù„Ø£Ø³ÙˆØ§Ù‚ Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
]
,
"Ø§Ù„Ø§Ø®ØªØ±Ø§Ø¹Ø§Øª_ÙˆØ§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§": [
    "Ù…Ø®ØªØ±Ø¹ÙˆÙ† Ø¹Ø±Ø¨ Ø¨Ø§Ù„ØµÙˆØ±", 
    "Ø§Ø¨ØªÙƒØ§Ø±Ø§Øª ØªÙ‚Ù†ÙŠØ© Ø¹Ø±Ø¨ÙŠØ©", 
    "ØµÙˆØ± Ø£Ø¬Ù‡Ø²Ø© Ø°ÙƒÙŠØ© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©", 
    "Ù…Ù„ØµÙ‚Ø§Øª Ø£Ø¯ÙˆØ§Øª ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©",
    "ØªØµØ§Ù…ÙŠÙ… Ø£Ø¯ÙˆØ§Øª Ø§Ø¨ØªÙƒØ§Ø±ÙŠØ©", 
    "ØµÙˆØ± Ø±ÙˆØ¨ÙˆØªØ§Øª Ù…Ø¹ ØªØ³Ù…ÙŠØ§Øª", 
    "Ù…Ø®Ø·Ø· ØªØ·ÙˆØ± Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§", 
    "Ø¨Ø·Ø§Ù‚Ø§Øª Ø£Ø¯ÙˆØ§Øª Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ©"
]
,
"Ø§Ù„Ù…Ù‡Ù†_ÙˆØ§Ù„ÙˆØ¸Ø§Ø¦Ù": [
    "ØµÙˆØ± Ø¨ÙŠØ¦Ø© Ø§Ù„Ø¹Ù…Ù„", 
    "Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ù‡Ù† Ø§Ù„Ù…Ø®ØªÙ„ÙØ©", 
    "Ø±Ø³Ù… ØªÙˆØ¶ÙŠØ­ÙŠ Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„",
    "Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ø­Ø±Ù Ø§Ù„ÙŠØ¯ÙˆÙŠØ©", 
    "Ø²ÙŠ Ø§Ù„Ø¹Ù…Ù„ Ø­Ø³Ø¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©", 
    "Ø±Ø³Ù… Ù…ÙˆØ¸Ù ÙÙŠ Ø¹Ù…Ù„Ù‡", 
    "Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ù„Ù„Ø£Ø·ÙØ§Ù„", 
    "ØªÙˆØ¶ÙŠØ­ Ø¨ÙŠØ¦Ø§Øª Ø¹Ù…Ù„ Ù…ØªÙ†ÙˆØ¹Ø©"
]
,
"Ø§Ù„Ø·Ø¹Ø§Ù…_ÙˆØ§Ù„Ø¹Ø§Ø¯Ø§Øª": [
    "Ø¹Ø§Ø¯Ø§Øª Ø§Ù„Ø£ÙƒÙ„ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©", 
    "ØµÙˆØ± Ø³ÙØ±Ø© Ø¹Ø±Ø¨ÙŠØ©", 
    "Ø¨Ø·Ø§Ù‚Ø§Øª Ø¢Ø¯Ø§Ø¨ Ø§Ù„Ù…Ø§Ø¦Ø¯Ø©",
    "Ø£Ø·Ø¨Ø§Ù‚ Ù…ÙˆØ³Ù…ÙŠØ© Ù…ØµÙˆØ±Ø©", 
    "Ù…Ù„ØµÙ‚Ø§Øª Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„ÙˆØ¬Ø¨Ø§Øª", 
    "Ø±Ø³Ù… Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø§Ø¦Ø¯Ø©", 
    "Ø¥Ù†ÙÙˆØ¬Ø±Ø§ÙÙŠÙƒ Ù‡Ø±Ù… ØºØ°Ø§Ø¦ÙŠ Ø¹Ø±Ø¨ÙŠ", 
    "ØµÙˆØ± Ø§Ù„Ø·Ù‡ÙŠ Ø§Ù„Ù…Ù†Ø²Ù„ÙŠ"
]
,
"Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø§Øª_ÙˆØ§Ù„Ø§Ø­ØªÙØ§Ù„Ø§Øª": [
    "Ø§Ø­ØªÙØ§Ù„Ø§Øª Ø§Ù„Ù†Ø¬Ø§Ø­ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠØ©", 
    "ØµÙˆØ± ÙŠÙˆÙ… Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¹Ø±Ø¨ÙŠ", 
    "Ø§Ø­ØªÙØ§Ù„Ø§Øª Ø¨ÙŠØ¦ÙŠØ© Ù…ØµÙˆØ±Ø©",
    "Ø¨Ø·Ø§Ù‚Ø§Øª ØªÙ‡Ù†Ø¦Ø© Ø¹Ø±Ø¨ÙŠØ©", 
    "Ù…Ù„ØµÙ‚Ø§Øª Ø£Ø¹ÙŠØ§Ø¯ ÙˆØ·Ù†ÙŠØ©", 
    "ØµÙˆØ± ØªØ²ÙŠÙŠÙ† Ø§Ù„Ø­ÙÙ„Ø§Øª", 
    "Ø£Ù„Ø¹Ø§Ø¨ Ø­ÙÙ„Ø§Øª Ø§Ù„Ø£Ø·ÙØ§Ù„", 
    "Ø¨Ø·Ø§Ù‚Ø§Øª ÙŠÙˆÙ… Ø§Ù„Ø£Ù… ÙˆØ§Ù„Ø£Ø¨"
]
,
"Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª_ÙˆØ§Ù„Ø·Ø¨ÙŠØ¹Ø©": [
    "Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø¨Ø±ÙŠØ© Ø¹Ø±Ø¨ÙŠØ©", 
    "ØµÙˆØ± Ø§Ù„Ø·ÙŠÙˆØ± Ø§Ù„Ù…Ø­Ù„ÙŠØ©", 
    "Ù…Ù„ØµÙ‚Ø§Øª Ø§Ù„Ø­Ø´Ø±Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©",
    "ØµÙˆØ± Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø§Ù„Ù…Ø²Ø±Ø¹Ø©", 
    "Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø§Ù„Ø¨Ø­Ø±ÙŠØ©", 
    "ØªØµÙ†ÙŠÙ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø¨ÙŠØ¦Ø©", 
    "Ø³Ù„ÙˆÙƒ Ø§Ù„Ø­ÙŠÙˆØ§Ù† Ø¨Ø§Ù„ØµÙˆØ±", 
    "ØµÙˆØ± Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø§Ù„Ù„ÙŠÙ„ ÙˆØ§Ù„Ù†Ù‡Ø§Ø±"
]
}

SAVE_ROOT = "arabic_images_4"
IMAGES_PER_SUBCATEGORY = 100
HEADERS = {'User-Agent': 'Mozilla/5.0'}
MAX_THREADS = 10

# ================================
# Scraper Functions
# ================================

def fetch_ddg_urls(query, max_results):
    urls = []
    try:
        with DDGS() as ddgs:
            for result in ddgs.images(query, max_results=max_results):
                image_url = result.get("image")
                if image_url:
                    urls.append(image_url)
    except Exception as e:
        print(f"[DuckDuckGo] Error for '{query}': {e}")
    return urls


def fetch_bing_urls(query, max_results):
    try:
        return bing_image_urls(query, limit=max_results)
    except Exception as e:
        print(f"[Bing] Failed for {query}: {e}")
        return []

def download_image(url, save_path):
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        if response.status_code == 200:
            image = Image.open(BytesIO(response.content)).convert("RGB")
            image.save(save_path, "JPEG")
            return True
    except Exception:
        pass
    return False

def download_task(args):
    url, save_path = args
    return download_image(url, save_path)

# ================================
# Main Logic
# ================================

def scrape_and_save_images():
    os.makedirs(SAVE_ROOT, exist_ok=True)

    for category, subcategories in categories.items():
        category_path = os.path.join(SAVE_ROOT, category)
        os.makedirs(category_path, exist_ok=True)

        for subcat in subcategories:
            print(f"\nðŸ”Ž Scraping: {subcat} in category {category}")
            # query = subcat
            query = f"{subcat} ØµÙˆØ± ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ù…ÙƒØªÙˆØ¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø¯ÙˆÙ† Ø¹Ù„Ø§Ù…Ø§Øª Ù…Ø§Ø¦ÙŠØ©"
            urls = set()

            urls.update(fetch_ddg_urls(query, IMAGES_PER_SUBCATEGORY))
            urls.update(fetch_bing_urls(query, IMAGES_PER_SUBCATEGORY))

            print(f"ðŸŒ Found {len(urls)} image URLs for {subcat}")

            download_tasks = []
            count = 0
            for url in urls:
                # if count >= IMAGES_PER_SUBCATEGORY:
                    # break
                filename = f"img_{subcat.replace(' ', '_')}_{str(count+1).zfill(4)}.jpg"
                save_path = os.path.join(category_path, filename)
                download_tasks.append((url, save_path))
                count += 1

            # Parallel Download
            print(f"â¬‡ Downloading {len(download_tasks)} images for {subcat} using {MAX_THREADS} threads...")
            success = 0
            with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
                results = list(executor.map(download_task, download_tasks))

            success = sum(results)
            print(f"âœ… Downloaded {success}/{len(download_tasks)} images for {subcat}")

# ================================
# Run Script
# ================================

if __name__ == "__main__":
    scrape_and_save_images()
